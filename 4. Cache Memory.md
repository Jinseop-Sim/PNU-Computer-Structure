# Cache Memory
---
## Cache란?
> Cache Memory는 기존 Main Memory의 더딘 속도 증가를 돕기 위해 고안된 보조 Memory이다.  

## Cache Capacity
- Words 또는 Bytes 단위로 표현된다.
- 1 Byte = 8 Bits
- Word Size
  - Organization의 기본 단위이다.
  - 8, 16, 32bit가 보통 크기이며 크게는 64bit까지도 있다.

### Transfer Unit
- Internal : Data Bus Width에 따라 전송된다.
- External : Word보다 큰 단위인 Block에 따라 전송된다.

### Addressable Unit
- Address로 사용될 수 있는 가장 작은 Location
- Word 또는 Byte 크기를 사용한다.
  - A = Adress의 길이(bit)
  - N = Addressable Unit의 갯수
  - N = 2^A 가 된다.
  - 예를 들어 32bit Address길이를 가지면, 2^32개의 Addressable Unit이 사용가능하다.

## Memory Access Method
> Memory에 있는 정보에 접근 하는 방법에 대해 알아보자.  
- Sequential
  - 각 Data는 고유 주소를 갖지 않는다.
  - 시작부터 끝까지 순서에 따라 Data를 읽게 된다. (Rewind도 가능)
  - Access time은 이전 Data의 Location과 읽으려는 Data의 위치에 영향을 받는다.
  - ex) Tape
- Direct
  - 각 Data는 이제부터 고유 위치를 갖는다.
  - 각 Sector들을 Jump로 넘어다니며, 각 Track을 Sequential하게 찾는다.
  - Access time은 이전 Data의 Location과 읽으려는 Data의 위치에 영향을 받는다.
  - ex) Disk
- Associative
  - 각 Data는 주소에 따라 명확하게 구별이 되어 있다.
  - Location들은 Random하게 주소가 배정되며, 직접적인 접근이 가능하다.
  - 따라서 Access Time은 이전 접근이나 위치에 영향을 받지 않는다.
  - ex) RAM

## Performace
- Access Time
  - Address를 찾는 동작을 시작으로 Data를 획득하기까지 걸리는 시간이다.
  - RAM의 경우, Data의 주소에 접근해서 Transfer이 완료되는 시점까지가 Access Time이다.
  - non-RAM의 경우, 원하는 위치에 Hardware을 위치시키는 것 까지가 Access Time이다.
- Memory Cycle Time
  - RAM에서 굉장히 중요하게 적용된다.
  - 다음 접근을 하기 전에 __Recover__ 하는 시간이 필요하다.
  - 따라서 Cycle Time = Access time + Recover time이 된다.
- Transfer Rate
  - Memory에서 Data가 in/out 될 때 전송되는 Data의 양을 말한다.
  - RAM은 보통 1/(Memory Cycle Time)bps 로 계산한다.
  - non-RAM은 보통 Tn = Ta + N/R 로 계산한다.
    - Tn : Average time to R/W N bits
    - Ta : Average Access Time
    - N : # of bits

## Physical Types
- Semiconductor(반도체) : RAM, ROM, Flash Memory..
- Magnetic(자기) : Disk, Tape(사장)
- Optical(광학) : CD, DVD, Blue Ray

### Physical Characteristics
- Decay(부패) : 물리적으로 녹슬거나 손상을 입는 매체인가?
- Volatility(휘발성) : 전기 신호가 꺼지면 기억이 지워지는가? (중요)
- Erasabiltiy : Readonly(ROM, Optical)인가? Read/Writable 한가?
- Power consumption : 전력 소비

## Memory Hierarchy
> Memory를 설계할 때 우리느 어떤 것을 고려하는가?  

- 결국 우리가 원하는 것은 Performance의 증가이다!
  - 빠른 Access Time ==> Cost가 비싸진다.
  - 큰 용량 ==> Cost가 싸진다. 하지만 Access Time이 길어진다.
  - 이 둘은 Trade-off 관계에 있어 항상 딜레마이다.
  - 따라서 우리는 Memory Hierarhcy를 생각하며, 어떻게 하면 두 마리 토끼를 잡을 수 있을지 고민해야 한다.  
  ![image](https://user-images.githubusercontent.com/71700079/161678732-3d2ac8bc-2105-4c7e-8845-141bb2c84ee5.png)  

### Locality of Reference
> 공간 및 시간적 지역성을 확보해서 Memory의 Average Access Time을 줄일 수 있다.  

- Spatial Locality
  - 접근했던 곳 주변의 Data들에 접근하려는 지역성.
  - 균일하게 모든 위치를 참조하는 것이 아닌, 특정 Cluster 형태의 부분만 집중적으로 참조하려는 경향.
  - ex) Sequential Instruction Access, Arrays, Tables
- Temporal Locality
  - 최근에 접근했던 Data들에 다시 접근하려는 지역성.
  - ex) Iteration Loop

### Hierarchy List
- __Register(CPU) > L1 Cache > L2 Cache > Main Memory > Disk Cache > Disk(virtual memory)__
- 보통은 Disk까지가 마지막이다. 그 이후에 Optical.. Tape..가 부가적으로 붙을 수 있다.  

### Performance Example
> 2단계로 이루어진 Memory System이 있다고 가정을 하고, 성능 계산을 해보자.  

- Level 1 : Access Time T1(L1 Cache)
- Level 2 : Access Time T2(L2 Cache)
  - 이 때, Hit Ratio(H) : T1에서 Reference까 끝날 확률. (opp. Miss Ratio)
  - Average Access Time = L1에서 끝날 확률 * T1 + L1에서 끝나지 않을 확률 * (T1+T2) = H * T1 + (1-H) * (T1+T2)  
                        = T1 + (1 - H)T2  
- 직접 숫자에 대입해서 계산을 해보자.
- Level 1 : 1 microsecs
- Level 2 : 10 microsecs
  - 이 때, Hit Ratio(H) : 95%라고 가정하자.
  - Average Access Time = 0.95 * 1microsecs + 0.05 * (1 + 10) = 1 + 0.05 * 10 = 1.5 microsecs  

### DRAM & SRAM
- 보통은 DRAM이 상용화가 되어있다.
  - 하지만 SRAM이 DRAM보다 10배 가량 빨라서, 보통 Cache의 용도로 사용한다.
    - CPU <=> SRAM(L1) <=> DRAM(Main Memory)
  - 그럼 SRAM을 Main Memory로 쓰면 안되나요?
    - 안된다! DRAM보다 100배 가량 비싸기 때문에, 우리는 쓸 수 없다.

## Cache/Main Memory Structure  
![image](https://user-images.githubusercontent.com/71700079/161680622-1d1dcb2f-a5d7-4763-98be-ac13e09bea68.png)  
- 위와 같은 구조로 CPU와 Memory간의 Data 교환이 일어난다.  
- 아래와 같은 알고리즘에 따라 Cache와 Main Memory에 접근한다.  
![image](https://user-images.githubusercontent.com/71700079/161680767-7707b17a-4c3b-46fe-ad8d-f26968be7b2f.png)  

### Cache/Main Memory Design
- Memory
  - n bits Address를 갖는다.(2^n개의 Addressable Unit)
  - M fixed length의 Block들이 있고, 한 Block에 K개의 Words로 구성된다.
  - 이 때 Block의 길이 M = 2^n / K가 된다.
- Cache
  - C개의 Slots(Lines)로 구성된다. (Memory의 Block과 같은 개념이다.)
  - 따라서 한 개의 Slot은 K개의 Words로 구성되어 있다.
  - Slots의 수 C는 Memory Block의 길이 M보다 월등히 작다. (C << M)  
![image](https://user-images.githubusercontent.com/71700079/161681815-d860b516-8699-40c6-82c3-c96c1af595cf.png)  

### Cache Design(Cont'd)
- 보통 1K ~ 512K까지 용량이 존재한다.
- 당연히 용량이 커질 수록 더 비싸진다.
- Mapping Function
  - Direct : 지정된 위치에 저장을 한다. (Simple, 하지만 성능이 낮다.)
  - Associate : 아무데나 Random하게 저장한다. (Complex, 하지만 높은 성능을 보인다.)
  - Set Associate : 둘을 절충한 방식. (현재 가장 많이 쓰이는 방식.)

#### Direct Function Example : 64KB Cache  
![image](https://user-images.githubusercontent.com/71700079/161682387-af267498-16eb-4b88-987e-758e0af1c125.png)  
- Cache의 각 Line은 4 Bytes라고 가정하자. 그럼 Cache는 64 / 4 = 16KB개의 Line을 갖게 된다.
- 16MB의 Main Memory가 있고, Byte Addressable하다고 가정하자.
  - Main Memory가 16MB라는 것은 24bit짜리 address를 쓰고 있다는 것이다. (2^24 = 16MB) 그리고 각 Block당 길이가 4MB라고 가정하자.
  - C = 16KB, M = 4MB, C << M  
![image](https://user-images.githubusercontent.com/71700079/162114301-5bac11aa-c857-4945-87a4-9d667f122a94.png)

#### Associate Function Example  
![image](https://user-images.githubusercontent.com/71700079/162114182-c82ed680-004f-42d7-8292-40c2a7ff9868.png)  
- Memory Block이 Cache의 아무데나 저장이 되는 방식이다.
- Direct Mapping에서의 Address는 8bit(Tag) / 14bit(Cache Line) / 2bit(Identifier)로 구성했다면
  - 이번에는 22bit(Tag) / 2bit(Identifier)로 구성한다.
- Pros
  - 굉장히 Flexible하다.(Utilization의 상승)
- Cons
  - 탐색 비용이 너무 비싸진다.
  - 동시에 모든 Line을 비교하려면 회로가 너무 복잡해진다.  
![image](https://user-images.githubusercontent.com/71700079/162114333-ab9bad41-39d0-4b77-b0ca-60013a38add7.png)  

#### Set Associate Function Example  
![image](https://user-images.githubusercontent.com/71700079/162114708-c9cf93cc-f2b1-4956-9d2e-57c21f3a3895.png)  
- 이전의 두 방식을 절충한 방식이다.
- Cache를 V개의 Set과 K개의 Line으로 나눈다.
  - # of Lines(m) = v * k
  - i = j mod v (i = Cache set number, j = memory block number)
  - K-way Set Associate 방식을 사용한다.(K가 set의 개수가 되며 보통 2, 4를 사용한다.)
    - 1-way Set Associate : Set당 Line의 수가 1이므로, Direct mapping과 동일해진다. 
    - m-way Set Associate : Set당 Line의 수가 M이므로, 하나의 Set만 존재한다. 이는 Fully Associative와 동일해진다.
- 위에서 계속 들고있는 예시를 계속 들어보면, 16MB Main Memory에서 24bit Address를 쓴다면
  - 9bit(Tag) / 13bit(Set Number) / 2bit(Identifier)로 구성이 될 것이다.
  - Set Number의 bit가 13bit가 된 이유는, 2-Way Set Associate 방식이라고 가정하고, 2^14 / 2 를 해야하기 때문이다.  
![image](https://user-images.githubusercontent.com/71700079/162114892-d189cce8-9f29-4a58-8c0e-12b75023e568.png)

### Cache Size와 K-way에 따른 Hit ratio  
![image](https://user-images.githubusercontent.com/71700079/162115857-9aaea7aa-d980-4f07-831d-8d0b9cfb2586.png)  
- 특정 수준이상 넘어가면, 일정한 성능을 보임을 알 수 있다.
